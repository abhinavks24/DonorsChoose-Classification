{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3ee9864-3d9c-40ee-9fb9-ce96b6fdf708",
   "metadata": {},
   "source": [
    "<h2>Summary of Classification implemented on Donor Choose DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f38667-b098-40a5-b418-cce204f4e565",
   "metadata": {},
   "source": [
    ">For the given Donor Choose data set we used multiple calassificationtechinques alon with various NLP vectorizers. Following are the performance results received:<br><br><b>Note</b>: We have only included the performance of the models which were obtained by using the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edfd1b29-80ac-404b-abfa-d7466104910a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+---------------------------+-----------+----------+\n",
      "|                Vectorizer               |           Model           | Train AUC | Test AUC |\n",
      "+-----------------------------------------+---------------------------+-----------+----------+\n",
      "|         TFIDF + sentiment score         | XGBM using LightGBM model |   0.9152  |  0.7146  |\n",
      "|               Word to Vec               | XGBM using LightGBM model |   0.8477  |  0.6844  |\n",
      "|                   BoW                   |       Multinomial NB      |   0.7854  |  0.701   |\n",
      "|                  TFIDF                  |       Multinomial NB      |   0.8375  |  0.6615  |\n",
      "|           TFIDF using unigrams          |       Decision Tree       |   0.6778  |  0.6386  |\n",
      "|  TFIDF using both unigrams and bigrams  |       Decision Tree       |   0.6768  |  0.6371  |\n",
      "|                TFIDF-W2V                |       Decision Tree       |   0.7257  |  0.6099  |\n",
      "| TFIDF using non-zero feature importance |       Decision Tree       |   0.6774  |  0.6332  |\n",
      "+-----------------------------------------+---------------------------+-----------+----------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Vectorizer\", \"Model\", \"Train AUC\", \"Test AUC\"]\n",
    "table.add_row([\"TFIDF + sentiment score\", \"XGBM using LightGBM model\", 0.9152,0.7146])\n",
    "table.add_row([\"Word to Vec\", \"XGBM using LightGBM model\", 0.8477,0.6844])\n",
    "table.add_row([\"BoW\", \"Multinomial NB\", 0.7854,0.7010])\n",
    "table.add_row([\"TFIDF\", \"Multinomial NB\", 0.8375,0.6615])\n",
    "table.add_row([\"TFIDF using unigrams\", \"Decision Tree\", 0.6778,0.6386])\n",
    "table.add_row([\"TFIDF using both unigrams and bigrams \", \"Decision Tree\", 0.6768,0.6371])\n",
    "table.add_row([\"TFIDF-W2V\", \"Decision Tree\", 0.7257,0.6099])\n",
    "table.add_row([\"TFIDF using non-zero feature importance\", \"Decision Tree\", 0.6774,0.6332])\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749865a4-6cdf-4085-8bec-0514b24deee5",
   "metadata": {},
   "source": [
    "><b>From the above table we can conclude that the usnig TF-IDF vectorisation and sentiment scores along wiht XGBoost Model(uning lightgbm implementaiton) have produces the best results.</b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
